# 1Ô∏è‚É£ Frequent Itemset Mining
# 2Ô∏è‚É£ Identify which products are most frequently bought together.
# ------------------------------------------------------------------------------
# Load Required Libraries
# ------------------------------------------------------------------------------
pacman::p_load("tidyverse", "lubridate", "arules", "arulesViz", "hms")
# ------------------------------------------------------------------------------
# Extract Data
# ------------------------------------------------------------------------------
# Load the CSV file
groceries <- read.csv("~/Downloads/Assignment.csv", sep = ";", row.names = NULL, check.names = FALSE, encoding = "UTF-8")
# Inspect the first few rows
head(groceries)
str(groceries)
# ------------------------------------------------------------------------------
# Data Cleaning & Transformation
# ------------------------------------------------------------------------------
# Convert date and time columns properly
groceries <- groceries |>
mutate(
Time = sub(".* ", "", Date),  # Extract time (HH:MM)
Date = sub(" .*", "", Date),  # Extract date (DD.MM.YYYY)
Date = as.Date(Date, format = "%d.%m.%Y"), # Convert Date to YYYY-MM-DD format
Time = hms::parse_hm(Time),
Price = as.numeric(gsub(",", ".", Price)), # Convert price to numeric
Price = ifelse(is.na(Price), 0, Price)  # Replace NA values in Price with 0
) |>
relocate(Time, .after = Date)  # Move Time after Date
# Check for missing values
colSums(is.na(groceries))
# Assign "Guest" to missing CustomerID values
groceries$CustomerID <- ifelse(is.na(groceries$CustomerID), "Guest", groceries$CustomerID)
# Remove invalid item names (e.g., "?")
groceries <- groceries |>
filter(!grepl("\\?|Adjust bad debt", Itemname)) |>  # Remove problematic entries
mutate(Itemname = trimws(Itemname))  # Remove leading/trailing spaces
# ------------------------------------------------------------------------------
# Convert Data into Transaction Format
# ------------------------------------------------------------------------------
# Format: "basket" (each row = one transaction, comma-separated items)
# Ensure every basket is properly structured
basket_data <- groceries |>
group_by(BillNo) |>
summarise(Items = paste(sort(unique(Itemname)), collapse = ",")) |>  # Join items correctly
filter(nchar(Items) > 0) |>  # Remove empty baskets
dplyr::pull(Items)
# Save cleaned data in the correct format
writeLines(basket_data, "groceries_basket_cleaned.csv")
# ------------------------------------------------------------------------------
# Load Transaction Data
# ------------------------------------------------------------------------------
# Load data in "basket" format (each line = one transaction, comma-separated)
basket_data_clean <- read.transactions("groceries_basket_cleaned.csv", format = "basket", sep = ",", header = FALSE, encoding = "UTF-8")
# Inspect the first few transactions
inspect(head(basket_data_clean, 4))
# ------------------------------------------------------------------------------
# Data Exploration
# ------------------------------------------------------------------------------
# Check total transactions
length(basket_data_clean)
# Check item frequency (most common items)
itemFrequencyPlot(basket_data_clean, topN = 10, type = "absolute", horiz = TRUE)
itemFrequencyPlot(basket_data_clean, topN = 10, type = "relative", horiz = TRUE)
# Sort items alphabetically and check the first few items
sorted_items <- sort(itemFrequency(basket_data_clean), decreasing = FALSE)
head(sorted_items)
# ------------------------------------------------------------------------------
# Market Basket Analysis - Apriori Algorithm
# ------------------------------------------------------------------------------
rules <- apriori(
basket_data_clean,
parameter = list(supp = 0.001, conf = 0.8, minlen = 2, maxlen = 4),
control = list(verbose = FALSE)
)
# Check number of generated rules
rules
# Sort and inspect the top rules by confidence, support, and lift
inspect(sort(rules, by = "lift", decreasing = TRUE)[1:5])
inspect(sort(rules, by = "confidence", decreasing = TRUE)[1:5])
inspect(sort(rules, by = "support", decreasing = TRUE)[1:5])
# ------------------------------------------------------------------------------
# Visualization
# ------------------------------------------------------------------------------
# Simplify rules for visualization
simplerules <- apriori(basket_data_clean, parameter = list(supp = 0.001, conf = 0.7, maxlen = 7), control = list(verbose = FALSE))
simplerules <- sort(simplerules, by = 'lift')[1:10] # Select top 10 rules
# Plot the rules
plot(simplerules, method = "graph", engine = "htmlwidget")
# ------------------------------------------------------------------------------
# End of Script
# ------------------------------------------------------------------------------
# will always give you a warning when you specify maxlen. This is simply
# letting us know it ran fine
# If someone buys x, they also buy y
# Dataset = g
# supp = must appears. at least in 0.1% in all transactions
# conf = the rule must be correct at least 80 % of time
# minlen = minimum 2 products
# maxlen = maximum 4 products
# control = detailed loggong on/off
rules <- apriori(basket_data_cleaned, parameter = list(supp = 0.001, conf = 0.80, minlen = 2,
maxlen = 4), control = list(verbose=FALSE)
)
# How many set of rules has it made
rules
pacman::p_load("tidyverse", "lubridate", "arules", "arulesViz", "hms")
# Extract the .CSV-file
groceries <- read.csv("~/Downloads/Assignment.csv", sep = ";", row.names = NULL, check.names = FALSE, encoding = "UTF-8")
# Inspecting groceries
head(groceries)
# Make every line into baskets, alphabetically order (format = basket)
basket_data_clean <- read.transactions("groceries_basket_cleaned.csv", format = "single", sep = ",", cols = c(1, 2), header = FALSE, encoding = "UTF8")
inspect(head(basket_data_clean,4))
pacman::p_load("tidyverse", "lubridate", "arules", "arulesViz", "hms")
# Extract the .CSV-file
groceries <- read.csv("~/Downloads/Assignment.csv", sep = ";", row.names = NULL, check.names = FALSE, encoding = "UTF-8")
# Inspecting groceries
head(groceries)
str(groceries)
groceries <- groceries |>
mutate(
Time = sub(".* ", "", Date),  # Extract time (HH:MM)
Date = sub(" .*", "", Date),  # Extract date (DD.MM.YYYY)
Date = as.Date(Date, format = "%d.%m.%Y"), # Convert Date to YYYY-MM-DD format
Time = hms::parse_hm(Time),
Price = as.numeric(gsub(",", ".", Price)),
Price = ifelse(is.na(Price), 0, Price)
) |>
relocate(Time, .after = Date)  # Move Time after Date
# Checking variables
str(groceries)
# Look at the NA values
colSums(is.na(groceries))
# Add Guest to the rows where CutomerID is missing
groceries$CustomerID <- ifelse(is.na(groceries$CustomerID), "Guest", groceries$CustomerID)
# Checking NA values again
colSums(is.na(groceries))
# Remove unwanted item names containing "?" before grouping
groceries <- groceries |>
filter(!grepl("\\?", Itemname)) |>  # Remove rows where Itemname contains "?"
mutate(Itemname = trimws(Itemname))  # Remove leading/trailing spaces
# Group items by BillNo and collapse them into a single row per transaction
basket_data <- groceries |>
group_by(BillNo) |>
summarise(Items = paste(sort(unique(Itemname[Itemname != "Adjust bad debt"])), collapse = ",")) |>
filter(nchar(Items) > 0) |>
dplyr::pull(Items)
# Save the cleaned file
writeLines(basket_data, "groceries_basket_cleaned.csv")
# Make every line into baskets, alphabetically order (format = basket)
basket_data_clean <- read.transactions("groceries_basket_cleaned.csv", format = "single", sep = ",", cols = c(1, 2), header = FALSE, encoding = "UTF8")
inspect(head(basket_data_clean,4))
inspect(head(basket_data_clean,4))
# Select only relevant columns and remove invalid entries
basket_data <- groceries |>
select(BillNo, Itemname) |>
filter(Itemname != "Adjust bad debt") |>  # Remove bad entries
filter(!grepl("\\?", Itemname)) |>  # Remove items with "?"
mutate(Itemname = trimws(Itemname)) |>  # Clean spaces
distinct()  # Remove duplicates
# Save cleaned file with correct format (BillNo, Itemname)
write.csv(basket_data, "groceries_basket_cleaned.csv", row.names = FALSE, quote = FALSE)
# Make every line into baskets, alphabetically order (format = basket)
basket_data_clean <- read.transactions("groceries_basket_cleaned.csv", format = "single", sep = ",", cols = c(1, 2), header = FALSE, encoding = "UTF8")
# Select only relevant columns and remove invalid entries
basket_data <- groceries |>
select(BillNo, Itemname) |>
filter(Itemname != "Adjust bad debt") |>  # Remove bad entries
filter(!grepl("\\?", Itemname)) |>  # Remove items with "?"
mutate(Itemname = trimws(Itemname)) |>  # Clean spaces
distinct()  # Remove duplicates
# Save cleaned file with correct format (BillNo, Itemname)
write.csv(basket_data, "groceries_basket_cleaned.csv", row.names = FALSE, quote = FALSE)
# Make every line into baskets, alphabetically order (format = basket)
basket_data_clean <- read.transactions(
"groceries_basket_cleaned.csv",
format = "single",  # <-- Correct format for BillNo and Itemname
sep = ",",
cols = c(1,2),  # Column 1 = BillNo, Column 2 = Itemname
header = TRUE,
encoding = "UTF-8"
)
# Inspect to verify it looks correct
inspect(head(basket_data_clean, 10))  # Should show 10 separate transactions
unique_items <- unique(groceries$Itemname)
print(unique_items[grep("\\d{5,}", unique_items)])  # Check if BillNo appears in Itemname
# ------------------------------------------------------------------------------
# Introduction
# ------------------------------------------------------------------------------
# # I want to analyse a dataset from Kaggle
#
#
# üîπ Beginner-Friendly Market Basket Analysis Tasks
#
# 1Ô∏è‚É£ Frequent Itemset Mining
#
# üõ†Ô∏è Task: Identify which products are most frequently bought together.
# üîç Why? This is the foundation of MBA‚Äîfinding patterns in customer transactions.
# üí° Steps:
#   ‚Ä¢	Clean the dataset (remove nulls, duplicate transactions).
# ‚Ä¢	Use Apriori algorithm or FP-Growth to find frequent itemsets.
# ‚Ä¢	Rank items by support (how often they appear together).
# ‚Ä¢	Interpret insights: Which items drive sales when bundled?
#
#   üìå Example Output:
#   ‚Ä¢	‚ÄúMilk & Bread‚Äù appear in 12% of transactions.
# ‚Ä¢	‚ÄúDiapers & Beer‚Äù (classic example) are frequently bought together.
# ------------------------------------------------------------------------------
# PACMAN
# ------------------------------------------------------------------------------
pacman::p_load("tidyverse", "lubridate", "arules", "arulesViz", "hms")
# ------------------------------------------------------------------------------
# Extract data
# ------------------------------------------------------------------------------
# Extract the .CSV-file
groceries <- read.csv("~/Downloads/Assignment.csv", sep = ";", row.names = NULL, check.names = FALSE, encoding = "UTF-8")
# Inspecting groceries
head(groceries)
str(groceries)
# ------------------------------------------------------------------------------
# Transform data
# ------------------------------------------------------------------------------
groceries <- groceries |>
mutate(
Time = sub(".* ", "", Date),  # Extract time (HH:MM)
Date = sub(" .*", "", Date),  # Extract date (DD.MM.YYYY)
Date = as.Date(Date, format = "%d.%m.%Y"), # Convert Date to YYYY-MM-DD format
Time = hms::parse_hm(Time),
Price = as.numeric(gsub(",", ".", Price)),
Price = ifelse(is.na(Price), 0, Price)
) |>
relocate(Time, .after = Date)  # Move Time after Date
# Checking variables
str(groceries)
# Look at the NA values
colSums(is.na(groceries))
# Add Guest to the rows where CutomerID is missing
groceries$CustomerID <- ifelse(is.na(groceries$CustomerID), "Guest", groceries$CustomerID)
# Checking NA values again
colSums(is.na(groceries))
# Remove unwanted item names containing "?" before grouping
groceries <- groceries |>
filter(!grepl("\\?", Itemname)) |>  # Remove rows where Itemname contains "?"
mutate(Itemname = trimws(Itemname))  # Remove leading/trailing spaces
# Remove unwanted item names before processing
basket_data <- groceries |>
select(BillNo, Itemname) |>
filter(Itemname != "Adjust bad debt") |>  # Remove bad entries
filter(!grepl("\\?", Itemname)) |>  # Remove rows where Itemname contains "?"
filter(!grepl("\\d{5,}", Itemname)) |>  # Remove wrongly coded item names
mutate(
Itemname = trimws(Itemname),  # Remove spaces
BillNo = as.character(BillNo)  # Ensure BillNo is treated as text, not numbers
) |>
distinct()  # Remove duplicates
# Save cleaned file
write.csv(basket_data, "groceries_basket_cleaned.csv", row.names = FALSE, quote = FALSE)
unique_items <- unique(groceries$Itemname)
print(unique_items[grep("\\d{5,}", unique_items)])  # Check if BillNo appears in Itemname
# plumber.R
library(plumber)
#* Plot a histogram
#* @serializer png
#* @get /plot
function() {
rand <- rnorm(100)
hist(rand)
}
# plumber.R
install.packages(plumber)
install.packages("plumber")
# plumber.R
library(plumber)
#* Plot a histogram
#* @serializer png
#* @get /plot
function() {
rand <- rnorm(100)
hist(rand)
}
# plumber.R
library(plumber)
#* Plot a histogram
#* @serializer png
#* @get /plot
function() {
rand <- rnorm(100)
hist(rand)
}
plumber::plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Data Engineering/plumber_de/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Data Engineering/plumber_de/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Data Engineering/plumber_de/plumber.R')$run()
# ------------------------------------------------------------------------------
# Pacman
# ------------------------------------------------------------------------------
pacman_pload(
plumber,
tidyverse
)
# ------------------------------------------------------------------------------
# Pacman
# ------------------------------------------------------------------------------
pacman_p_load(
plumber,
tidyverse
)
# ------------------------------------------------------------------------------
# Pacman
# ------------------------------------------------------------------------------
pacman_load(
plumber,
tidyverse
)
# ------------------------------------------------------------------------------
# Pacman
# ------------------------------------------------------------------------------
pacman::p_load(
plumber,
tidyverse
)
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Data Engineering/plumber_de/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Data Engineering/plumber_de/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Data Engineering/plumber_de/plumber.R')$run()
shiny::runApp('Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Datavisualisering/praktik_app')
runApp('Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Datavisualisering/praktik_app')
# 1. Test om Google Sheets-data stadig hentes korrekt
test_data <- read_sheet(sheet_url)
head(test_data)
# 2. Tjek dine milj√∏variabler
Sys.getenv("AUTH0_USER")
Sys.getenv("AUTH0_KEY")
Sys.getenv("AUTH0_SECRET")
# 3. Genstart din app UDEN Auth0:
shinyApp(ui, server)
runApp('Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Datavisualisering/praktik_app')
shiny::runApp('Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Datavisualisering/praktik_app')
runApp('Library/CloudStorage/OneDrive-EaDania/Dataanalyse/Datavisualisering/praktik_app')
# Minimalistisk Shiny-app til automatisk rens af CSV
library(shiny)
library(readr)
library(dplyr)
library(janitor)
ui <- fluidPage(
titlePanel("CSV Cleaner"),
sidebarLayout(
sidebarPanel(
fileInput("file", "Upload din CSV-fil", accept = ".csv"),
downloadButton("download", "Download renset fil")
),
mainPanel(
tableOutput("preview")
)
)
)
server <- function(input, output, session) {
# Reaktiv data der renses automatisk
cleaned_data <- reactive({
req(input$file)
tryCatch({
df <- read_csv(input$file$datapath, show_col_types = FALSE)
df_clean <- df %>%
janitor::clean_names() %>%                       # Rens kolonnenavne
mutate(across(everything(), ~na_if(., ""))) %>% # Konverter tomme felter til NA
filter(if_any(everything(), ~!is.na(.)))         # Fjern helt tomme r√¶kker
return(df_clean)
}, error = function(e) {
showNotification(paste("Fejl ved indl√¶sning:", e$message), type = "error")
return(NULL)
})
})
# Vis preview
output$preview <- renderTable({
head(cleaned_data(), 10)
})
# Download renset CSV
output$download <- downloadHandler(
filename = function() {
paste0("renset_data_", Sys.Date(), ".csv")
},
content = function(file) {
write_csv(cleaned_data(), file)
}
)
}
shinyApp(ui, server)
plumber::plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
plumb(file='Library/CloudStorage/OneDrive-EaDania/Dataanalyse/GitHub/test_exam_april_2025/countries_statistic/plumber.R')$run()
shiny::runApp('Desktop/test')
# Load required packages
library(shiny)
library(bslib)
library(echarts4r)
runApp('Desktop/test')
runApp('Desktop/test')
runApp('Desktop/test')
runApp('Desktop/test')
runApp('Desktop/test')
runApp('Desktop/test')
# ------------------------------------------------------------------------------
# 10. Visualiseringer
# -------------------------------------------------------------------------
# Plot modeller efter deres performance
# -------------------------------------------------------------------------
# Din tibble, hvis ikke du allerede har den i en variabel:
metrics_df <- tibble::tibble(
wflow_id = c("churn_recipe_rf", "churn_recipe_xgboost", "churn_recipe_svm_rbf",
"churn_recipe_logistic", "churn_recipe_knn", "churn_recipe_naive_bayes"),
accuracy = c(0.825, 0.827, 0.845, 0.807, 0.743, 0.710),
f_meas   = c(0.724, 0.723, 0.708, 0.697, 0.592, 0.287),
roc_auc  = c(0.877, 0.881, 0.439, 0.858, 0.778, 0.855),
sens     = c(0.777, 0.766, 0.643, 0.755, 0.634, 0.272),
spec     = c(0.845, 0.852, 0.929, 0.828, 0.788, 0.893)
)
# G√∏r labels lidt p√¶nere
metrics_focus <- metrics_long %>%
filter(metric %in% c("accuracy", "f_meas", "roc_auc")) %>%
mutate(metric = case_when(
metric == "accuracy" ~ "Accuracy",
metric == "f_meas" ~ "F1-score",
metric == "roc_auc" ~ "ROC AUC",
TRUE ~ metric
))
# ------------------------------------------------------------------------------
# 10. Visualiseringer
# -------------------------------------------------------------------------
# Plot modeller efter deres performance
# -------------------------------------------------------------------------
# Din tibble, hvis ikke du allerede har den i en variabel:
metrics_df <- tibble::tibble(
wflow_id = c("churn_recipe_rf", "churn_recipe_xgboost", "churn_recipe_svm_rbf",
"churn_recipe_logistic", "churn_recipe_knn", "churn_recipe_naive_bayes"),
accuracy = c(0.825, 0.827, 0.845, 0.807, 0.743, 0.710),
f_meas   = c(0.724, 0.723, 0.708, 0.697, 0.592, 0.287),
roc_auc  = c(0.877, 0.881, 0.439, 0.858, 0.778, 0.855),
sens     = c(0.777, 0.766, 0.643, 0.755, 0.634, 0.272),
spec     = c(0.845, 0.852, 0.929, 0.828, 0.788, 0.893)
)
# G√∏r labels lidt p√¶nere
metrics_focus <- metrics_long |>
filter(metric %in% c("accuracy", "f_meas", "roc_auc")) |>
mutate(metric = case_when(
metric == "accuracy" ~ "Accuracy",
metric == "f_meas" ~ "F1-score",
metric == "roc_auc" ~ "ROC AUC",
TRUE ~ metric
))
# ------------------------------------------------------------------------------
# 10. Visualiseringer
# -------------------------------------------------------------------------
# Plot modeller efter deres performance
# -------------------------------------------------------------------------
# Din tibble, hvis ikke du allerede har den i en variabel:
metrics_df <- tibble::tibble(
wflow_id = c("churn_recipe_rf", "churn_recipe_xgboost", "churn_recipe_svm_rbf",
"churn_recipe_logistic", "churn_recipe_knn", "churn_recipe_naive_bayes"),
accuracy = c(0.825, 0.827, 0.845, 0.807, 0.743, 0.710),
f_meas   = c(0.724, 0.723, 0.708, 0.697, 0.592, 0.287),
roc_auc  = c(0.877, 0.881, 0.439, 0.858, 0.778, 0.855),
sens     = c(0.777, 0.766, 0.643, 0.755, 0.634, 0.272),
spec     = c(0.845, 0.852, 0.929, 0.828, 0.788, 0.893)
)
# G√∏r labels lidt p√¶nere
metrics_focus <- metrics_df |>
filter(metric %in% c("accuracy", "f_meas", "roc_auc")) |>
mutate(metric = case_when(
metric == "accuracy" ~ "Accuracy",
metric == "f_meas" ~ "F1-score",
metric == "roc_auc" ~ "ROC AUC",
TRUE ~ metric
))
# Din tibble, hvis ikke du allerede har den i en variabel:
metrics_df <- tibble::tibble(
wflow_id = c("churn_recipe_rf", "churn_recipe_xgboost", "churn_recipe_svm_rbf",
"churn_recipe_logistic", "churn_recipe_knn", "churn_recipe_naive_bayes"),
accuracy = c(0.825, 0.827, 0.845, 0.807, 0.743, 0.710),
f_meas   = c(0.724, 0.723, 0.708, 0.697, 0.592, 0.287),
roc_auc  = c(0.877, 0.881, 0.439, 0.858, 0.778, 0.855),
sens     = c(0.777, 0.766, 0.643, 0.755, 0.634, 0.272),
spec     = c(0.845, 0.852, 0.929, 0.828, 0.788, 0.893)
)
ggplot(metrics_focus, aes(x = metric, y = value, fill = metric)) +
geom_col(show.legend = FALSE) +
geom_text(aes(label = round(value, 3)), vjust = -0.3, size = 3.5) +
facet_wrap(~ wflow_id) +
ylim(0, 1.05) +
labs(
title = "Model performance (Accuracy, F1 og ROC AUC)",
x = NULL,
y = "Score"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 0),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold")
) +
scale_fill_brewer(palette = "Set3")
setwd("~/OneDrive - EaDania/Dataanalyse/GitHub/2_semester_opgave")
